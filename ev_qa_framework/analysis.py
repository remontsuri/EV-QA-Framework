"""EV QA Analysis: ML-based battery telemetry and quality assurance.
–ú–æ–¥—É–ª—å –º–∞—à–∏–Ω–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è –¥–ª—è –¥–µ—Ç–µ–∫—Ü–∏–∏ –∞–Ω–æ–º–∞–ª–∏–π –≤ —Ç–µ–ª–µ–º–µ—Ç—Ä–∏–∏ –±–∞—Ç–∞—Ä–µ–∏.
"""

import numpy as np
import pandas as pd
from sklearn.ensemble import IsolationForest
from sklearn.preprocessing import StandardScaler
from typing import Dict, Optional, Tuple
import warnings

warnings.filterwarnings('ignore')


class EVBatteryAnalyzer:
    """
    ML-–∞–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä —Ç–µ–ª–µ–º–µ—Ç—Ä–∏–∏ –±–∞—Ç–∞—Ä–µ–∏ EV –Ω–∞ –æ—Å–Ω–æ–≤–µ –∞–ª–≥–æ—Ä–∏—Ç–º–∞ Isolation Forest.
    
    Isolation Forest ‚Äî —ç—Ç–æ –∞–ª–≥–æ—Ä–∏—Ç–º –æ–±–Ω–∞—Ä—É–∂–µ–Ω–∏—è –∞–Ω–æ–º–∞–ª–∏–π, –∫–æ—Ç–æ—Ä—ã–π –∏–∑–æ–ª–∏—Ä—É–µ—Ç –≤—ã–±—Ä–æ—Å—ã
    –ø—É—Ç–µ–º —Å–ª—É—á–∞–π–Ω–æ–≥–æ –≤—ã–±–æ—Ä–∞ –ø—Ä–∏–∑–Ω–∞–∫–∞ –∏ –∑–∞—Ç–µ–º —Å–ª—É—á–∞–π–Ω–æ–≥–æ –≤—ã–±–æ—Ä–∞ –∑–Ω–∞—á–µ–Ω–∏—è —Ä–∞–∑–¥–µ–ª–µ–Ω–∏—è
    –º–µ–∂–¥—É –º–∞–∫—Å–∏–º—É–º–æ–º –∏ –º–∏–Ω–∏–º—É–º–æ–º –≤—ã–±—Ä–∞–Ω–Ω–æ–≥–æ –ø—Ä–∏–∑–Ω–∞–∫–∞. –ê–Ω–æ–º–∞–ª–∏–∏ –∏–∑–æ–ª–∏—Ä—É—é—Ç—Å—è –±—ã—Å—Ç—Ä–µ–µ,
    —á–µ–º –Ω–æ—Ä–º–∞–ª—å–Ω—ã–µ —Ç–æ—á–∫–∏ –¥–∞–Ω–Ω—ã—Ö.
    
    Attributes:
        model: –ú–æ–¥–µ–ª—å IsolationForest –∏–∑ scikit-learn
        scaler: StandardScaler –¥–ª—è –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏–∏ –¥–∞–Ω–Ω—ã—Ö
        anomalies: DataFrame —Å –æ–±–Ω–∞—Ä—É–∂–µ–Ω–Ω—ã–º–∏ –∞–Ω–æ–º–∞–ª–∏—è–º–∏
        contamination: –î–æ–ª—è –∞–Ω–æ–º–∞–ª–∏–π –≤ –¥–∞—Ç–∞—Å–µ—Ç–µ (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é 0.1 = 10%)
    """
    
    def __init__(self, contamination: float = 0.1, n_estimators: int = 200, random_state: int = 42):
        """
        –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –∞–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä–∞ —Ç–µ–ª–µ–º–µ—Ç—Ä–∏–∏.
        
        Args:
            contamination: –û–∂–∏–¥–∞–µ–º–∞—è –¥–æ–ª—è –∞–Ω–æ–º–∞–ª–∏–π –≤ –¥–∞–Ω–Ω—ã—Ö (0.0 - 1.0).
                          –ù–∞–ø—Ä–∏–º–µ—Ä, 0.1 –æ–∑–Ω–∞—á–∞–µ—Ç, —á—Ç–æ ~10% –¥–∞–Ω–Ω—ã—Ö –º–æ–≥—É—Ç –±—ã—Ç—å –∞–Ω–æ–º–∞–ª—å–Ω—ã–º–∏.
            n_estimators: –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –¥–µ—Ä–µ–≤—å–µ–≤ –≤ –∞–Ω—Å–∞–º–±–ª–µ (–±–æ–ª—å—à–µ = —Ç–æ—á–Ω–µ–µ, –Ω–æ –º–µ–¥–ª–µ–Ω–Ω–µ–µ).
                         –†–µ–∫–æ–º–µ–Ω–¥—É–µ—Ç—Å—è 100-200 –¥–ª—è –±–∞–ª–∞–Ω—Å–∞ —Ç–æ—á–Ω–æ—Å—Ç–∏ –∏ —Å–∫–æ—Ä–æ—Å—Ç–∏.
            random_state: Seed –¥–ª—è –≤–æ—Å–ø—Ä–æ–∏–∑–≤–æ–¥–∏–º–æ—Å—Ç–∏ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤.
        
        –ü—Ä–∏–º–µ—á–∞–Ω–∏–µ:
            - contamination –≤–ª–∏—è–µ—Ç –Ω–∞ —á—É–≤—Å—Ç–≤–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å: –º–µ–Ω—å—à–µ –∑–Ω–∞—á–µ–Ω–∏–µ = –º–µ–Ω—å—à–µ –ª–æ–∂–Ω—ã—Ö —Å—Ä–∞–±–∞—Ç—ã–≤–∞–Ω–∏–π
            - n_estimators —Ä–µ–∫–æ–º–µ–Ω–¥—É–µ—Ç—Å—è 100+ –¥–ª—è —Å—Ç–∞–±–∏–ª—å–Ω—ã—Ö —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
        """
        # –°–æ–∑–¥–∞–µ–º –º–æ–¥–µ–ª—å Isolation Forest —Å –Ω–∞—Å—Ç—Ä–æ–µ–Ω–Ω—ã–º–∏ –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º–∏
        self.model = IsolationForest(
            contamination=contamination,    # –û–∂–∏–¥–∞–µ–º–∞—è –¥–æ–ª—è –∞–Ω–æ–º–∞–ª–∏–π
            n_estimators=n_estimators,      # –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –¥–µ—Ä–µ–≤—å–µ–≤ (–±–æ–ª—å—à–µ = —Å—Ç–∞–±–∏–ª—å–Ω–µ–µ)
            max_samples='auto',             # –ê–≤—Ç–æ-–≤—ã–±–æ—Ä —Ä–∞–∑–º–µ—Ä–∞ –ø–æ–¥–≤—ã–±–æ—Ä–∫–∏
            random_state=random_state,      # –î–ª—è –≤–æ—Å–ø—Ä–æ–∏–∑–≤–æ–¥–∏–º–æ—Å—Ç–∏
            n_jobs=-1                       # –ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –≤—Å–µ CPU —è–¥—Ä–∞
        )
        
        # StandardScaler –Ω–æ—Ä–º–∞–ª–∏–∑—É–µ—Ç –¥–∞–Ω–Ω—ã–µ: (x - mean) / std
        # –≠—Ç–æ –≤–∞–∂–Ω–æ, —Ç–∞–∫ –∫–∞–∫ IsolationForest —á—É–≤—Å—Ç–≤–∏—Ç–µ–ª–µ–Ω –∫ –º–∞—Å—à—Ç–∞–±—É –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
        self.scaler = StandardScaler()
        
        # –•—Ä–∞–Ω–∏–ª–∏—â–µ –¥–ª—è –æ–±–Ω–∞—Ä—É–∂–µ–Ω–Ω—ã—Ö –∞–Ω–æ–º–∞–ª–∏–π (–∑–∞–ø–æ–ª–Ω—è–µ—Ç—Å—è –ø–æ—Å–ª–µ analyze_telemetry)
        self.anomalies: Optional[pd.DataFrame] = None
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º contamination –¥–ª—è –¥–æ—Å—Ç—É–ø–∞ –∏–∑–≤–Ω–µ
        self.contamination = contamination
        
    def analyze_telemetry(self, df_telemetry: pd.DataFrame) -> Dict[str, any]:
        """
        –ê–Ω–∞–ª–∏–∑ —Ç–µ–ª–µ–º–µ—Ç—Ä–∏–∏ –±–∞—Ç–∞—Ä–µ–∏ –Ω–∞ –ø—Ä–µ–¥–º–µ—Ç –∞–Ω–æ–º–∞–ª–∏–π.
        
        –ê–ª–≥–æ—Ä–∏—Ç–º:
        1. –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è –¥–∞–Ω–Ω—ã—Ö —á–µ—Ä–µ–∑ StandardScaler (–ø—Ä–∏–≤–µ–¥–µ–Ω–∏–µ –∫ –æ–¥–Ω–æ–π —à–∫–∞–ª–µ)
        2. –û–±—É—á–µ–Ω–∏–µ IsolationForest –Ω–∞ –Ω–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö
        3. –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ –∞–Ω–æ–º–∞–ª–∏–π (-1 = –∞–Ω–æ–º–∞–ª–∏—è, 1 = –Ω–æ—Ä–º–∞)
        4. –†–∞—Å—á–µ—Ç anomaly scores (—á–µ–º –º–µ–Ω—å—à–µ, —Ç–µ–º –±–æ–ª–µ–µ –∞–Ω–æ–º–∞–ª—å–Ω–∞—è —Ç–æ—á–∫–∞)
        5. –û—Ü–µ–Ω–∫–∞ —Å–µ—Ä—å–µ–∑–Ω–æ—Å—Ç–∏ –Ω–∞ –æ—Å–Ω–æ–≤–µ –º–∏–Ω–∏–º–∞–ª—å–Ω–æ–≥–æ score
        
        Args:
            df_telemetry: DataFrame —Å –∫–æ–ª–æ–Ω–∫–∞–º–∏ ['voltage', 'current', 'temp', 'soc'].
                         –ö–∞–∂–¥–∞—è —Å—Ç—Ä–æ–∫–∞ ‚Äî —ç—Ç–æ –æ–¥–∏–Ω –º–æ–º–µ–Ω—Ç –≤—Ä–µ–º–µ–Ω–∏.
        
        Returns:
            –°–ª–æ–≤–∞—Ä—å —Å —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞–º–∏ –∞–Ω–∞–ª–∏–∑–∞:
                - total_samples: –û–±—â–µ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ç–æ—á–µ–∫ –¥–∞–Ω–Ω—ã—Ö
                - anomalies_detected: –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –æ–±–Ω–∞—Ä—É–∂–µ–Ω–Ω—ã—Ö –∞–Ω–æ–º–∞–ª–∏–π
                - anomaly_percentage: –ü—Ä–æ—Ü–µ–Ω—Ç –∞–Ω–æ–º–∞–ª–∏–π –æ—Ç –æ–±—â–µ–≥–æ —á–∏—Å–ª–∞
                - severity: –£—Ä–æ–≤–µ–Ω—å —Å–µ—Ä—å–µ–∑–Ω–æ—Å—Ç–∏ ('CRITICAL', 'WARNING', 'INFO')
        
        –ü—Ä–∏–º–µ—Ä:
            >>> df = pd.DataFrame({
            ...     'voltage': [48, 48, 200],  # 200 ‚Äî –∞–Ω–æ–º–∞–ª–∏—è
            ...     'current': [100, 100, 100],
            ...     'temp': [35, 35, 35],
            ...     'soc': [85, 85, 85]
            ... })
            >>> analyzer = EVBatteryAnalyzer()
            >>> results = analyzer.analyze_telemetry(df)
            >>> print(results['anomalies_detected'])
            1
        """
        # –®–∞–≥ 1: –í—ã–±–∏—Ä–∞–µ–º —Ç–æ–ª—å–∫–æ —á–∏—Å–ª–æ–≤—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏ –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞
        # SOC –Ω–µ –∏—Å–ø–æ–ª—å–∑—É–µ–º –¥–ª—è –¥–µ—Ç–µ–∫—Ü–∏–∏, —Ç–∞–∫ –∫–∞–∫ —ç—Ç–æ –∑–∞–≤–∏—Å–∏–º–∞—è –ø–µ—Ä–µ–º–µ–Ω–Ω–∞—è
        features = ['voltage', 'current', 'temp']
        X = df_telemetry[features]
        
        # –®–∞–≥ 2: –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è –¥–∞–Ω–Ω—ã—Ö (mean=0, std=1)
        # –≠—Ç–æ –∫—Ä–∏—Ç–∏—á–µ—Å–∫–∏ –≤–∞–∂–Ω–æ –¥–ª—è IsolationForest, —á—Ç–æ–±—ã –≤—Å–µ —Ñ–∏—á–∏ –∏–º–µ–ª–∏ –æ–¥–∏–Ω–∞–∫–æ–≤—ã–π –≤–µ—Å
        X_scaled = self.scaler.fit_transform(X)
        
        # –®–∞–≥ 3: –û–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏ –∏ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ –∞–Ω–æ–º–∞–ª–∏–π
        # fit_predict –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç: 1 –¥–ª—è –Ω–æ—Ä–º–∞–ª—å–Ω—ã—Ö —Ç–æ—á–µ–∫, -1 –¥–ª—è –∞–Ω–æ–º–∞–ª–∏–π
        predictions = self.model.fit_predict(X_scaled)
        
        # –®–∞–≥ 4: –†–∞—Å—á–µ—Ç anomaly scores (—á–µ–º –Ω–∏–∂–µ, —Ç–µ–º –±–æ–ª–µ–µ –∞–Ω–æ–º–∞–ª—å–Ω–∞—è —Ç–æ—á–∫–∞)
        # score_samples –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Å—Ä–µ–¥–Ω–∏–π path length –≤ –¥–µ—Ä–µ–≤—å—è—Ö
        # –ù–æ—Ä–º–∞–ª—å–Ω—ã–µ —Ç–æ—á–∫–∏: score –±–ª–∏–∂–µ –∫ 0
        # –ê–Ω–æ–º–∞–ª–∏–∏: score << 0 (–Ω–∞–ø—Ä–∏–º–µ—Ä, -0.5, -1.0)
        anomaly_scores = self.model.score_samples(X_scaled)
        
        # –®–∞–≥ 5: –§–∏–ª—å—Ç—Ä—É–µ–º –∞–Ω–æ–º–∞–ª–∏–∏ (–≥–¥–µ prediction == -1)
        self.anomalies = df_telemetry[predictions == -1].copy()
        
        # –î–æ–±–∞–≤–ª—è–µ–º anomaly scores –≤ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –¥–ª—è –¥–∞–ª—å–Ω–µ–π—à–µ–≥–æ –∞–Ω–∞–ª–∏–∑–∞
        if len(self.anomalies) > 0:
            self.anomalies['anomaly_score'] = anomaly_scores[predictions == -1]
        
        # –®–∞–≥ 6: –§–æ—Ä–º–∏—Ä—É–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç –∞–Ω–∞–ª–∏–∑–∞
        return {
            'total_samples': len(df_telemetry),
            'anomalies_detected': len(self.anomalies),
            'anomaly_percentage': (len(self.anomalies) / len(df_telemetry)) * 100,
            'severity': self._assess_severity(anomaly_scores)
        }
    
    def _assess_severity(self, scores: np.ndarray) -> str:
        """
        –û—Ü–µ–Ω–∫–∞ —É—Ä–æ–≤–Ω—è —Å–µ—Ä—å–µ–∑–Ω–æ—Å—Ç–∏ –æ–±–Ω–∞—Ä—É–∂–µ–Ω–Ω—ã—Ö –∞–Ω–æ–º–∞–ª–∏–π.
        
        –õ–æ–≥–∏–∫–∞ –æ—Ü–µ–Ω–∫–∏:
        - CRITICAL: –ï—Å—Ç—å —ç–∫—Å—Ç—Ä–µ–º–∞–ª—å–Ω—ã–µ –≤—ã–±—Ä–æ—Å—ã (score < -0.8)
                   –¢—Ä–µ–±—É–µ—Ç—Å—è –Ω–µ–º–µ–¥–ª–µ–Ω–Ω–æ–µ –≤–Ω–∏–º–∞–Ω–∏–µ ‚Äî –≤–æ–∑–º–æ–∂–Ω–∞ –∫—Ä–∏—Ç–∏—á–µ—Å–∫–∞—è –Ω–µ–∏—Å–ø—Ä–∞–≤–Ω–æ—Å—Ç—å
        - WARNING: –£–º–µ—Ä–µ–Ω–Ω—ã–µ –∞–Ω–æ–º–∞–ª–∏–∏ (score < -0.5)
                  –¢—Ä–µ–±—É–µ—Ç—Å—è –ø—Ä–æ–≤–µ—Ä–∫–∞ ‚Äî –≤–æ–∑–º–æ–∂–Ω–∞ –¥–µ–≥—Ä–∞–¥–∞—Ü–∏—è —Å–∏—Å—Ç–µ–º—ã
        - INFO: –°–ª–∞–±—ã–µ –∞–Ω–æ–º–∞–ª–∏–∏ –∏–ª–∏ –∏—Ö –æ—Ç—Å—É—Ç—Å—Ç–≤–∏–µ (score >= -0.5)
               –°–∏—Å—Ç–µ–º–∞ –≤ –Ω–æ—Ä–º–µ, –∞–Ω–æ–º–∞–ª–∏–∏ –Ω–µ–∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω—ã
        
        Args:
            scores: –ú–∞—Å—Å–∏–≤ anomaly scores –∏–∑ IsolationForest
        
        Returns:
            –°—Ç—Ä–æ–∫–∞ —Å —É—Ä–æ–≤–Ω–µ–º —Å–µ—Ä—å–µ–∑–Ω–æ—Å—Ç–∏: 'CRITICAL', 'WARNING' –∏–ª–∏ 'INFO'
        
        –ü—Ä–∏–º–µ—á–∞–Ω–∏–µ:
            –ü–æ—Ä–æ–≥–∏ (-0.8, -0.5) –ø–æ–¥–æ–±—Ä–∞–Ω—ã —ç–º–ø–∏—Ä–∏—á–µ—Å–∫–∏ –∏ –º–æ–≥—É—Ç –∫–æ—Ä—Ä–µ–∫—Ç–∏—Ä–æ–≤–∞—Ç—å—Å—è
            –ø–æ–¥ –∫–æ–Ω–∫—Ä–µ—Ç–Ω—É—é —Å–∏—Å—Ç–µ–º—É –Ω–∞ –æ—Å–Ω–æ–≤–µ –∏—Å—Ç–æ—Ä–∏—á–µ—Å–∫–∏—Ö –¥–∞–Ω–Ω—ã—Ö.
        """
        min_score = np.min(scores)
        
        if min_score < -0.8:
            return 'CRITICAL'  # –≠–∫—Å—Ç—Ä–µ–º–∞–ª—å–Ω–∞—è –∞–Ω–æ–º–∞–ª–∏—è ‚Äî –∫—Ä–∏—Ç–∏—á–µ—Å–∫–∏–π —É—Ä–æ–≤–µ–Ω—å
        elif min_score < -0.5:
            return 'WARNING'   # –£–º–µ—Ä–µ–Ω–Ω–∞—è –∞–Ω–æ–º–∞–ª–∏—è ‚Äî –ø—Ä–µ–¥—É–ø—Ä–µ–∂–¥–µ–Ω–∏–µ
        return 'INFO'          # –°–ª–∞–±–∞—è –∞–Ω–æ–º–∞–ª–∏—è –∏–ª–∏ –Ω–æ—Ä–º–∞


class AnomalyDetector(EVBatteryAnalyzer):
    """
    –†–∞—Å—à–∏—Ä–µ–Ω–Ω—ã–π –∫–ª–∞—Å—Å-–¥–µ—Ç–µ–∫—Ç–æ—Ä –∞–Ω–æ–º–∞–ª–∏–π —Å —Ä–∞–∑–¥–µ–ª—å–Ω—ã–º–∏ –º–µ—Ç–æ–¥–∞–º–∏ train/detect.
    
    –≠—Ç–æ—Ç –∫–ª–∞—Å—Å –ø–æ–∑–≤–æ–ª—è–µ—Ç:
    1. –û–±—É—á–∏—Ç—å –º–æ–¥–µ–ª—å –Ω–∞ "–Ω–æ—Ä–º–∞–ª—å–Ω—ã—Ö" –¥–∞–Ω–Ω—ã—Ö (train)
    2. –ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –æ–±—É—á–µ–Ω–Ω—É—é –º–æ–¥–µ–ª—å –¥–ª—è –¥–µ—Ç–µ–∫—Ü–∏–∏ –Ω–∞ –Ω–æ–≤—ã—Ö –¥–∞–Ω–Ω—ã—Ö (detect)
    
    –≠—Ç–æ –ø–æ–ª–µ–∑–Ω–æ –≤ –ø—Ä–æ–¥–∞–∫—à–µ–Ω–µ, –∫–æ–≥–¥–∞ –º–æ–¥–µ–ª—å –æ–±—É—á–∞–µ—Ç—Å—è –æ–¥–∏–Ω —Ä–∞–∑ –Ω–∞ –∏—Å—Ç–æ—Ä–∏—á–µ—Å–∫–∏—Ö
    –¥–∞–Ω–Ω—ã—Ö, –∞ –∑–∞—Ç–µ–º –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –¥–ª—è real-time –¥–µ—Ç–µ–∫—Ü–∏–∏.
    """
    
    def __init__(self, contamination: float = 0.01, n_estimators: int = 200, random_state: int = 42):
        """
        –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –¥–µ—Ç–µ–∫—Ç–æ—Ä–∞ –∞–Ω–æ–º–∞–ª–∏–π.
        
        Args:
            contamination: –û–∂–∏–¥–∞–µ–º–∞—è –¥–æ–ª—è –∞–Ω–æ–º–∞–ª–∏–π (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é 0.01 = 1%).
                          –î–ª—è –æ–±—É—á–µ–Ω–∏—è –Ω–∞ "—á–∏—Å—Ç—ã—Ö" –¥–∞–Ω–Ω—ã—Ö –∏—Å–ø–æ–ª—å–∑—É–π—Ç–µ –º–∞–ª–æ–µ –∑–Ω–∞—á–µ–Ω–∏–µ.
            n_estimators: –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –¥–µ—Ä–µ–≤—å–µ–≤ (—Ä–µ–∫–æ–º–µ–Ω–¥—É–µ—Ç—Å—è 200 –¥–ª—è —Å—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç–∏).
            random_state: Seed –¥–ª—è –≤–æ—Å–ø—Ä–æ–∏–∑–≤–æ–¥–∏–º–æ—Å—Ç–∏.
        """
        super().__init__(contamination, n_estimators, random_state)
        self._is_trained = False  # –§–ª–∞–≥ –æ–±—É—á–µ–Ω–Ω–æ—Å—Ç–∏ –º–æ–¥–µ–ª–∏
    
    def train(self, data: pd.DataFrame) -> None:
        """
        –û–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏ –Ω–∞ "–Ω–æ—Ä–º–∞–ª—å–Ω—ã—Ö" –¥–∞–Ω–Ω—ã—Ö.
        
        –†–µ–∫–æ–º–µ–Ω–¥—É–µ—Ç—Å—è –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –¥–∞–Ω–Ω—ã–µ –±–µ–∑ –∞–Ω–æ–º–∞–ª–∏–π –¥–ª—è –æ–±—É—á–µ–Ω–∏—è,
        —á—Ç–æ–±—ã –º–æ–¥–µ–ª—å –Ω–∞—É—á–∏–ª–∞—Å—å —Ä–∞—Å–ø–æ–∑–Ω–∞–≤–∞—Ç—å –Ω–æ—Ä–º–∞–ª—å–Ω–æ–µ –ø–æ–≤–µ–¥–µ–Ω–∏–µ –±–∞—Ç–∞—Ä–µ–∏.
        
        Args:
            data: DataFrame —Å –∫–æ–ª–æ–Ω–∫–∞–º–∏ ['voltage', 'current', 'temp', 'soc'].
                  –î–∞–Ω–Ω—ã–µ –¥–æ–ª–∂–Ω—ã —Å–æ–¥–µ—Ä–∂–∞—Ç—å –ø—Ä–µ–∏–º—É—â–µ—Å—Ç–≤–µ–Ω–Ω–æ –Ω–æ—Ä–º–∞–ª—å–Ω—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è.
        
        –ü—Ä–∏–º–µ—Ä:
            >>> normal_data = pd.DataFrame({
            ...     'voltage': np.random.normal(48, 1, 1000),
            ...     'current': np.random.normal(100, 5, 1000),
            ...     'temp': np.random.normal(35, 2, 1000),
            ...     'soc': np.random.normal(85, 5, 1000)
            ... })
            >>> detector = AnomalyDetector()
            >>> detector.train(normal_data)
        """
        features = ['voltage', 'current', 'temp']
        X = data[features]
        
        # –û–±—É—á–∞–µ–º scaler –Ω–∞ –Ω–æ—Ä–º–∞–ª—å–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö
        X_scaled = self.scaler.fit_transform(X)
        
        # –û–±—É—á–∞–µ–º IsolationForest
        self.model.fit(X_scaled)
        self._is_trained = True
        print(f"‚úÖ –ú–æ–¥–µ–ª—å –æ–±—É—á–µ–Ω–∞ –Ω–∞ {len(data)} —Ç–æ—á–∫–∞—Ö –¥–∞–Ω–Ω—ã—Ö")
    
    def detect(self, data: pd.DataFrame) -> Tuple[np.ndarray, np.ndarray]:
        """
        –î–µ—Ç–µ–∫—Ü–∏—è –∞–Ω–æ–º–∞–ª–∏–π –Ω–∞ –Ω–æ–≤—ã—Ö –¥–∞–Ω–Ω—ã—Ö —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º –æ–±—É—á–µ–Ω–Ω–æ–π –º–æ–¥–µ–ª–∏.
        
        Args:
            data: DataFrame —Å –Ω–æ–≤–æ–π —Ç–µ–ª–µ–º–µ—Ç—Ä–∏–µ–π –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞.
        
        Returns:
            –ö–æ—Ä—Ç–µ–∂ (predictions, scores):
                - predictions: –ú–∞—Å—Å–∏–≤ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π (-1 = –∞–Ω–æ–º–∞–ª–∏—è, 1 = –Ω–æ—Ä–º–∞)
                - scores: –ú–∞—Å—Å–∏–≤ anomaly scores
        
        Raises:
            ValueError: –ï—Å–ª–∏ –º–æ–¥–µ–ª—å –Ω–µ –æ–±—É—á–µ–Ω–∞ (–Ω—É–∂–Ω–æ —Å–Ω–∞—á–∞–ª–∞ –≤—ã–∑–≤–∞—Ç—å train)
        
        –ü—Ä–∏–º–µ—Ä:
            >>> new_data = pd.DataFrame({
            ...     'voltage': [48, 200],  # 200 ‚Äî –∞–Ω–æ–º–∞–ª–∏—è
            ...     'current': [100, 100],
            ...     'temp': [35, 35],
            ...     'soc': [85, 85]
            ... })
            >>> predictions, scores = detector.detect(new_data)
            >>> print(predictions)  # [1, -1]
        """
        if not self._is_trained:
            raise ValueError("–ú–æ–¥–µ–ª—å –Ω–µ –æ–±—É—á–µ–Ω–∞! –°–Ω–∞—á–∞–ª–∞ –≤—ã–∑–æ–≤–∏—Ç–µ –º–µ—Ç–æ–¥ train()")
        
        features = ['voltage', 'current', 'temp']
        X = data[features]
        
        # –ü—Ä–∏–º–µ–Ω—è–µ–º —É–∂–µ –æ–±—É—á–µ–Ω–Ω—ã–π scaler
        X_scaled = self.scaler.transform(X)
        
        # –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ –Ω–∞ –Ω–æ–≤—ã—Ö –¥–∞–Ω–Ω—ã—Ö
        predictions = self.model.predict(X_scaled)
        scores = self.model.score_samples(X_scaled)
        
        anomaly_count = np.sum(predictions == -1)
        print(f"üîç –û–±–Ω–∞—Ä—É–∂–µ–Ω–æ –∞–Ω–æ–º–∞–ª–∏–π: {anomaly_count}/{len(data)}")
        
        return predictions, scores


if __name__ == '__main__':
    # –ü—Ä–∏–º–µ—Ä –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è EVBatteryAnalyzer
    print("=== –¢–µ—Å—Ç EVBatteryAnalyzer ===")
    analyzer = EVBatteryAnalyzer()
    
    # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º —Ç–µ—Å—Ç–æ–≤—É—é —Ç–µ–ª–µ–º–µ—Ç—Ä–∏—é
    np.random.seed(42)
    data = {
        'voltage': np.random.normal(48, 2, 1000),
        'current': np.random.normal(100, 15, 1000),
        'temp': np.random.normal(35, 5, 1000),
        'soc': np.random.normal(85, 10, 1000)
    }
    df = pd.DataFrame(data)
    
    # –ê–Ω–∞–ª–∏–∑
    results = analyzer.analyze_telemetry(df)
    print(f"–ê–Ω–∞–ª–∏–∑ –∑–∞–≤–µ—Ä—à–µ–Ω: {results}")
    print(f"–ê–Ω–æ–º–∞–ª–∏–π: {results['anomalies_detected']}/{results['total_samples']}")
    print(f"–°–µ—Ä—å–µ–∑–Ω–æ—Å—Ç—å: {results['severity']}")
    
    # –ü—Ä–∏–º–µ—Ä –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è AnomalyDetector
    print("\n=== –¢–µ—Å—Ç AnomalyDetector (train/detect) ===")
    detector = AnomalyDetector(contamination=0.01, n_estimators=200)
    
    # –û–±—É—á–µ–Ω–∏–µ –Ω–∞ –Ω–æ—Ä–º–∞–ª—å–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö
    normal_data = pd.DataFrame({
        'voltage': np.random.normal(48, 1, 500),
        'current': np.random.normal(100, 5, 500),
        'temp': np.random.normal(35, 2, 500),
        'soc': np.random.normal(85, 5, 500)
    })
    detector.train(normal_data)
    
    # –î–µ—Ç–µ–∫—Ü–∏—è –Ω–∞ –Ω–æ–≤—ã—Ö –¥–∞–Ω–Ω—ã—Ö —Å –∞–Ω–æ–º–∞–ª–∏–µ–π
    test_data = pd.DataFrame({
        'voltage': [48, 48, 200, 48],  # 200V ‚Äî —è–≤–Ω–∞—è –∞–Ω–æ–º–∞–ª–∏—è
        'current': [100, 100, 100, 100],
        'temp': [35, 35, 35, 35],
        'soc': [85, 85, 85, 85]
    })
    predictions, scores = detector.detect(test_data)
    print(f"–ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è: {predictions}")
    print(f"Scores: {scores}")

